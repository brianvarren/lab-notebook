Today is a really exciting and important day for me. I’m working on the Lung Loop Sampler, which is an idea that has evolved over several iterations, first described in the original post. It started as a Reactor instrument that I was deeply in love with, and now I’m bringing it to life as hardware.

The major milestone today is the waveform view on the display finally becoming a reality. I have a wide 256×64 waveform view that is perfect for displaying sample waveforms in real time, complete with loop points and playback head position.

Leading up to this, I’ve been working on subsystems—setting up display driver functionality and getting the encoder menu system working. The current working example goes like this: on startup, the module runs through its load sequence, indexes the files on the SD card, and displays the list. You can scroll through the list with the rotary encoder, and it updates smoothly on the display. It feels really intuitive and satisfying. Clicking the encoder loads a file. The module reads it, detects peaks, normalizes to –3 dB, and converts it to Q15 format (signed 16-bit integers). That sets us up for fixed-point processing before PWM output. Right before output, we scale to 12 bits to preserve as much information as possible for effects, then quantize before sending to PWM. This leaves the door open for eventually moving to a codec with high-quality stereo audio, but for now, PWM is cheap, fast, and lets us get started without heavy implementation.

My process for this project has relied heavily on two AI models to help write source code. I’m also learning and applying intermediate to advanced embedded software engineering principles as I go. The project now has about 25 source files, each a few hundred lines long. That’s much larger than any module I’ve done before. For comparison, my Wave module was about 700 lines total. I’ve kept the main sketch under 300 lines, with supporting files ranging anywhere from 5 to 300 lines. The directory structure is easier to navigate, and I’m learning how to group code logically so everything stays modular. This also makes it easier to work with AI: instead of feeding the entire project at once—which runs into context limits—I can provide just the files relevant to the subsystem I’m working on.

Switching between GPT and Claude has also been useful. Both are great at writing and reasoning, but in long conversations they eventually start to hallucinate or regress, producing broken code. It feels like dubbing a tape over and over again—eventually the signal degrades. My solution is to push a feature as far as I can with one model, then hand the relevant files over to the other for a “fresh perspective.” Each model sees things the other missed. Bouncing between the two creates progress greater than the sum of their parts.

This is by far the biggest firmware project I’ve attempted, and it’s teaching me a lot about architecture and workflow. Getting the waveform view alive on the screen is proof that the pieces are connecting and that the Lung Sampler is becoming what I imagined.